# selection_parameters_parameter_tuning_worker.py
import logging
from PySide6.QtCore import Signal, QThread
import pandas as pd
from sklearn.model_selection import train_test_split, RandomizedSearchCV
from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier, RandomForestRegressor, GradientBoostingRegressor
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import (
    accuracy_score, f1_score, precision_score, recall_score,
    roc_auc_score, r2_score, mean_squared_error, mean_absolute_error, explained_variance_score
)
from sklearn.preprocessing import LabelEncoder
from .selection_of_parameters_logic import get_random_grid, get_random_search_params

logger = logging.getLogger(__name__)


class ParameterTuningWorker(QThread):
    progress_updated = Signal(float, int, int)
    tuning_completed = Signal(object, dict, float, str)
    error_occurred = Signal(str)
    info_message = Signal(str) 

    def __init__(self, parent=None, dataset_path=None, target_variable=None, model_type="", task_type="classification",
                 df=None, df_train=None, df_test=None):
        super().__init__(parent)
        self.dataset_path = dataset_path
        self.target_variable = target_variable
        self.model_type = model_type
        self.task_type = task_type
        self.df = df
        self.df_train = df_train
        self.df_test = df_test
        self._is_running = False
        self._should_stop = False 
        
    def stop(self):
        """
        –ú—è–≥–∫–∞—è –æ—Å—Ç–∞–Ω–æ–≤–∫–∞ –ø–æ—Ç–æ–∫–∞. –í—ã–∑—ã–≤–∞–µ—Ç—Å—è –∏–∑ GUI.
        """
        self._should_stop = True
        self.info_message.emit("–û—Å—Ç–∞–Ω–æ–≤–∫–∞ –ø–æ –∑–∞–ø—Ä–æ—Å—É –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è...")

    def run(self):
        if self._is_running:
            logger.warning("ParameterTuningWorker —É–∂–µ –∑–∞–ø—É—â–µ–Ω ‚Äî –ø—Ä–æ–ø—É—Å–∫.")
            return
        self._is_running = True
        self._should_stop = False  # –°–±—Ä–æ—Å —Ñ–ª–∞–≥–∞

        try:
            logger.info("=== –ó–∞–ø—É—Å–∫ –ø–æ–¥–±–æ—Ä–∞ –≥–∏–ø–µ—Ä–ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤ ===")

            # –ü—Ä–æ–≤–µ—Ä–∫–∞ –ø—Ä–µ—Ä—ã–≤–∞–Ω–∏—è –ø–µ—Ä–µ–¥ –Ω–∞—á–∞–ª–æ–º
            if self._should_stop:
                self.error_occurred.emit("–û–±—É—á–µ–Ω–∏–µ –æ—Ç–º–µ–Ω–µ–Ω–æ –¥–æ –Ω–∞—á–∞–ª–∞.")
                return

            # === 1. –ó–∞–≥—Ä—É–∑–∫–∞ –¥–∞–Ω–Ω—ã—Ö ===
            X_train, X_test, y_train, y_test = None, None, None, None

            # üîπ –°—Ü–µ–Ω–∞—Ä–∏–π 1: –ø–µ—Ä–µ–¥–∞–Ω—ã df_train –∏ df_test
            if self.df_train is not None and self.df_test is not None:
                logger.info("–ò—Å–ø–æ–ª—å–∑—É—é—Ç—Å—è –ø–µ—Ä–µ–¥–∞–Ω–Ω—ã–µ df_train –∏ df_test")
                X_train_full = self.df_train.drop(columns=[self.target_variable], errors='ignore')
                X_test_full = self.df_test.drop(columns=[self.target_variable], errors='ignore')
                y_train = self.df_train[self.target_variable].copy()
                y_test = self.df_test[self.target_variable].copy()

            # üîπ –°—Ü–µ–Ω–∞—Ä–∏–π 2: –ø–µ—Ä–µ–¥–∞–Ω df ‚Üí —Ä–∞–∑–±–∏–≤–∞–µ–º
            elif self.df is not None:
                logger.info("–ò—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è –ø–µ—Ä–µ–¥–∞–Ω–Ω—ã–π df, –¥–µ–ª–µ–Ω–∏–µ –Ω–∞ train/test")
                df = self.df
                if self.target_variable not in df.columns:
                    raise ValueError(f"–¶–µ–ª–µ–≤–∞—è –ø–µ—Ä–µ–º–µ–Ω–Ω–∞—è '{self.target_variable}' –Ω–µ –Ω–∞–π–¥–µ–Ω–∞")
                X_full = df.drop(columns=[self.target_variable])
                y_full = df[self.target_variable].copy()
                X_train_full, X_test_full, y_train, y_test = train_test_split(
                    X_full, y_full, test_size=0.2, random_state=42,
                    stratify=y_full if self.task_type == "classification" else None
                )

            # üîπ –°—Ü–µ–Ω–∞—Ä–∏–π 3: –ø–æ dataset_path
            elif self.dataset_path:
                logger.info(f"–ó–∞–≥—Ä—É–∑–∫–∞ –¥–∞–Ω–Ω—ã—Ö –∏–∑ —Ñ–∞–π–ª–∞: {self.dataset_path}")
                df = pd.read_csv(self.dataset_path)
                if self.target_variable not in df.columns:
                    raise ValueError(f"–¶–µ–ª–µ–≤–∞—è –ø–µ—Ä–µ–º–µ–Ω–Ω–∞—è '{self.target_variable}' –Ω–µ –Ω–∞–π–¥–µ–Ω–∞")
                X_full = df.drop(columns=[self.target_variable])
                y_full = df[self.target_variable].copy()
                X_train_full, X_test_full, y_train, y_test = train_test_split(
                    X_full, y_full, test_size=0.2, random_state=42,
                    stratify=y_full if self.task_type == "classification" else None
                )
            else:
                raise ValueError("–ù–µ –ø–µ—Ä–µ–¥–∞–Ω—ã –Ω–∏ df, –Ω–∏ df_train/df_test, –Ω–∏ dataset_path")

            if self._should_stop:
                self.error_occurred.emit("–û–±—É—á–µ–Ω–∏–µ –ø—Ä–µ—Ä–≤–∞–Ω–æ –Ω–∞ —ç—Ç–∞–ø–µ –∑–∞–≥—Ä—É–∑–∫–∏ –¥–∞–Ω–Ω—ã—Ö.")
                return

            # === üîé –û–°–¢–ê–í–õ–Ø–ï–ú –¢–û–õ–¨–ö–û –ß–ò–°–õ–û–í–´–ï –ü–†–ò–ó–ù–ê–ö–ò ===
            X_train = X_train_full.select_dtypes(include=['number'])
            X_test = X_test_full.select_dtypes(include=['number'])
            dropped_columns = X_train_full.columns.difference(X_train.columns).tolist()

            if dropped_columns:
                msg = f"–ü—Ä–æ–ø—É—â–µ–Ω—ã –Ω–µ—á–∏—Å–ª–æ–≤—ã–µ –ø—Ä–∏–∑–Ω–∞–∫–∏: {', '.join(dropped_columns)}"
                self.info_message.emit(msg)
                logger.info(msg)
            else:
                self.info_message.emit("–í—Å–µ –ø—Ä–∏–∑–Ω–∞–∫–∏ ‚Äî —á–∏—Å–ª–æ–≤—ã–µ.")
                logger.info("–ù–µ—á–∏—Å–ª–æ–≤—ã–µ –ø—Ä–∏–∑–Ω–∞–∫–∏ –Ω–µ –Ω–∞–π–¥–µ–Ω—ã.")

            if self._should_stop:
                self.error_occurred.emit("–û–±—É—á–µ–Ω–∏–µ –ø—Ä–µ—Ä–≤–∞–Ω–æ –ø–æ—Å–ª–µ –æ—á–∏—Å—Ç–∫–∏ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤.")
                return

            # === –û–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ —Ç–∏–ø–∞ –∑–∞–¥–∞—á–∏ ===
            if self.task_type == "classification":
                is_classification = True
            elif self.task_type == "regression":
                is_classification = False
            else:
                is_classification = (
                    y_train.dtype == 'object' or
                    y_train.nunique() < 20 or
                    self.model_type in ["RandomForestClassifier", "GradientBoostingClassifier", "LogisticRegression"]
                )
            task_type = "classification" if is_classification else "regression"

            # –ö–æ–¥–∏—Ä—É–µ–º y –ø—Ä–∏ –Ω–µ–æ–±—Ö–æ–¥–∏–º–æ—Å—Ç–∏
            if is_classification and y_train.dtype == 'object':
                le = LabelEncoder()
                y_train = le.fit_transform(y_train)
                y_test = le.transform(y_test)  # ‚ö†Ô∏è –ò—Å–ø–æ–ª—å–∑—É–µ–º —Ç–æ—Ç –∂–µ encoder

            if self._should_stop:
                self.error_occurred.emit("–û–±—É—á–µ–Ω–∏–µ –ø—Ä–µ—Ä–≤–∞–Ω–æ –¥–æ —Å–æ–∑–¥–∞–Ω–∏—è –º–æ–¥–µ–ª–∏.")
                return

            # === –ó–∞–≥—Ä—É–∑–∫–∞ –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤ ===
            params = get_random_search_params()
            grid = get_random_grid()
            hyperparams = grid.get(self.model_type)
            if not hyperparams:
                raise ValueError(f"–ù–µ—Ç –≥–∏–ø–µ—Ä–ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤ –¥–ª—è: {self.model_type}")

            # === –°–æ–∑–¥–∞–Ω–∏–µ –º–æ–¥–µ–ª–∏ ===
            model_classes = {
                'RandomForestClassifier': RandomForestClassifier,
                'GradientBoostingClassifier': GradientBoostingClassifier,
                'LogisticRegression': LogisticRegression,
                'RandomForestRegressor': RandomForestRegressor,
                'GradientBoostingRegressor': GradientBoostingRegressor,
            }
            model_cls = model_classes.get(self.model_type)
            if not model_cls:
                raise ValueError(f"–ù–µ–ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ–º–∞—è –º–æ–¥–µ–ª—å: {self.model_type}")
            estimator = model_cls(random_state=params['random_state'])

            # === –ú–µ—Ç—Ä–∏–∫–∏ ===
            if task_type == "classification":
                scoring = params['scoring']
                refit = params['refit']
                n_classes = len(pd.unique(y_train))
                if n_classes > 2:
                    scoring = {name: f'roc_auc_ovr' if name == 'roc_auc' else metric for name, metric in scoring.items()}
            else:
                scoring = params.get('scoring_regression', {
                    'r2': 'r2',
                    'neg_mean_squared_error': 'neg_mean_squared_error',
                    'neg_mean_absolute_error': 'neg_mean_absolute_error'
                })
                refit = params['refit']

            # === –û–±—É—á–µ–Ω–∏–µ ===
            n_iter = params['n_iter']
            cv = params['cv']
            verbose = params['verbose']
            n_jobs = params['n_jobs']
            random_state = params['random_state']

            search = RandomizedSearchCV(
                estimator=estimator,
                param_distributions=hyperparams,
                n_iter=n_iter,
                cv=cv,
                scoring=scoring,
                refit=refit,
                random_state=random_state,
                verbose=verbose,
                n_jobs=n_jobs
            )

            self.progress_updated.emit(0.0, 0, n_iter)

            # –ü—Ä–æ–≤–µ—Ä–∫–∞ –ø–µ—Ä–µ–¥ –æ–±—É—á–µ–Ω–∏–µ–º
            if self._should_stop:
                self.error_occurred.emit("–û–±—É—á–µ–Ω–∏–µ –ø—Ä–µ—Ä–≤–∞–Ω–æ –ø–µ—Ä–µ–¥ –∑–∞–ø—É—Å–∫–æ–º RandomizedSearchCV.")
                return

            # –ó–∞–ø—É—Å–∫ –æ–±—É—á–µ–Ω–∏—è
            search.fit(X_train, y_train)

            # –ü—Ä–æ–≤–µ—Ä–∫–∞ –ø–æ—Å–ª–µ –æ–±—É—á–µ–Ω–∏—è
            if self._should_stop:
                self.error_occurred.emit("–û–±—É—á–µ–Ω–∏–µ –±—ã–ª–æ –ø—Ä–µ—Ä–≤–∞–Ω–æ –≤–æ –≤—Ä–µ–º—è –ø–æ–¥–±–æ—Ä–∞ –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤.")
                return

            self.progress_updated.emit(100.0, n_iter, n_iter)

            # === –û—Ü–µ–Ω–∫–∞ ===
            model = search.best_estimator_
            pred = model.predict(X_test)

            if task_type == "classification":
                acc = accuracy_score(y_test, pred)
                f1 = f1_score(y_test, pred, average='macro', zero_division=0)
                prec = precision_score(y_test, pred, average='macro', zero_division=0)
                rec = recall_score(y_test, pred, average='macro', zero_division=0)
                roc_auc = 0.0
                if hasattr(model, "predict_proba"):
                    if len(set(y_test)) == 2:
                        proba = model.predict_proba(X_test)[:, 1]
                        roc_auc = roc_auc_score(y_test, proba)
                    else:
                        proba = model.predict_proba(X_test)
                        roc_auc = roc_auc_score(y_test, proba, average='weighted', multi_class='ovr')
                metrics = (
                    f"Accuracy: {acc:.4f}\n"
                    f"F1 Macro: {f1:.4f}\n"
                    f"Precision Macro: {prec:.4f}\n"
                    f"Recall Macro: {rec:.4f}\n"
                    f"ROC AUC: {roc_auc:.4f}"
                )
                primary_metric = roc_auc if roc_auc > 0 else acc
            else:
                r2 = r2_score(y_test, pred)
                mse = mean_squared_error(y_test, pred)
                mae = mean_absolute_error(y_test, pred)
                evs = explained_variance_score(y_test, pred)
                metrics = (
                    f"R¬≤ Score: {r2:.4f}\n"
                    f"Mean Squared Error: {mse:.4f}\n"
                    f"Mean Absolute Error: {mae:.4f}\n"
                    f"Explained Variance: {evs:.4f}"
                )
                primary_metric = r2

            # –û—Ç–ø—Ä–∞–≤–ª—è–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã
            self.tuning_completed.emit(model, search.best_params_, primary_metric, metrics)

        except Exception as e:
            if not self._should_stop:
                logger.error(f"–û—à–∏–±–∫–∞: {e}")
                self.error_occurred.emit(str(e))
            else:
                logger.info("–û–±—É—á–µ–Ω–∏–µ –±—ã–ª–æ –ø—Ä–µ—Ä–≤–∞–Ω–æ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–µ–º ‚Äî –æ—à–∏–±–∫–∞ –ø–æ–¥–∞–≤–ª–µ–Ω–∞.")
        finally:
            self._is_running = False
            # –°–∏–≥–Ω–∞–ª—ã –º–æ–∂–Ω–æ –æ—Ç–ø—Ä–∞–≤–ª—è—Ç—å, –Ω–æ –Ω–µ –≤—ã–∑—ã–≤–∞–µ–º deleteLater() –∑–¥–µ—Å—å ‚Äî —ç—Ç–æ –¥–µ–ª–∞–µ—Ç GUI
